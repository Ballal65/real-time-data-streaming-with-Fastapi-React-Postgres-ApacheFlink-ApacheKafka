FROM apache/airflow:2.7.1-python3.11

# Switch to root user to install system dependencies
USER root

# Install system-level dependencies
RUN apt-get update && \
    apt-get install -y gcc python3-dev openjdk-17-jdk procps && \
    apt-get clean

# Ensure the airflow group and user exist
RUN if ! getent group airflow >/dev/null; then groupadd -g 50000 airflow; fi && \
    if ! id -u airflow >/dev/null 2>&1; then useradd -u 50000 -g airflow airflow; fi

# Create the /opt/airflow/spark_jobs directory and set permissions
RUN mkdir -p /opt/airflow/spark_jobs && \
    chown -R airflow:airflow /opt/airflow/spark_jobs && \
    chmod -R 775 /opt/airflow/spark_jobs

# Set JAVA_HOME environment variable for Spark
ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
ENV PATH="$JAVA_HOME/bin:$PATH"

# Switch back to the airflow user
USER airflow

# Install required Python packages with specific versions
COPY ./airflow/requirements.txt /requirements.txt
RUN pip install --no-cache-dir -r /requirements.txt
